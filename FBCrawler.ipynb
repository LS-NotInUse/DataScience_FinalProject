{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "\n",
    "class FbPostCrawler:\n",
    "    def __init__(self, page_name, since=None, until=None):\n",
    "        self.access_token = \"&access_token=\" + \"EAACEdEose0cBANjv76sZA5bV6LavQCfOPHPn0CUANB0KJZC4pjsHjfZAOvHCBsZCqZALoZCNKuPOTc4ejzCH9OsMFq3vw6cDWwhGAaFYNu4Add2EsFw3BtbyZBcY4o6SJZC1gmusOJv09Dkp1e2PRxFEnR3GuSZCd8ys52lg7guBEeGXQ3dG8OUpK89gWl4KQMPzwOUZACJSCrgwZDZD\"\n",
    "        self.graph_url = \"https://graph.facebook.com/v3.0/\"\n",
    "        self.page_url = self.graph_url + \"%s/posts?\" %page_name\n",
    "        self.since, self.until = since, until\n",
    "        try:\n",
    "            self.file_name = \"post_\"+self.since+\"_\"+self.until+\".txt\"    #讀寫檔用\n",
    "        except:\n",
    "            self.file_name = \"post_all.txt\"\n",
    "        self.all_posts = list()    #紀錄所有貼文\n",
    "        \n",
    "    def getPosts(self, next_posts=None):\n",
    "        time_range = \"%s%s\" %(\"\"if self.since is None else \"&since=\"+self.since, \"\"if self.until is None else \"&until=\"+self.until)    #設定起訖時間\n",
    "        final_url = (self.page_url+ self.access_token + time_range) if next_posts is None else next_posts    #遞迴用url\n",
    "#         print(final_url)\n",
    "        \n",
    "        posts_json = json.loads(requests.get(final_url).text)\n",
    "        for data in posts_json[\"data\"]:\n",
    "            self.all_posts.append(data)\n",
    "            \n",
    "        if ((next_posts is not None) or ((self.since is not None) and (self.until is not None))):\n",
    "            try:\n",
    "                self.getPosts(posts_json[\"paging\"][\"next\"])    #遞迴直到沒有next\n",
    "            except:\n",
    "                print(\"get post done\")\n",
    "        \n",
    "    def printPosts(self):\n",
    "        if(len(self.all_posts) == 0):\n",
    "            print(\"empty\")\n",
    "            return\n",
    "        print(\"count: \" + str(len(self.all_posts)))\n",
    "        for element in self.all_posts:\n",
    "            print(element[\"created_time\"])\n",
    "    \n",
    "    def writePosts(self):\n",
    "            with open(self.file_name, \"w\") as writer:\n",
    "                writer.write(json.dumps(self.all_posts))\n",
    "            print(\"write done\")\n",
    "    \n",
    "    def readPosts(self, other_file=None):\n",
    "        with open(self.file_name if other_file is None else other_file, \"r\") as reader:\n",
    "            self.all_posts = json.load(reader)\n",
    "        print(\"read done\")\n",
    "        \n",
    "#######################################################\n",
    "    def getComments(self, next_comments= None):\n",
    "        for post in self.all_posts:\n",
    "            all_comments_list = list()\n",
    "            \n",
    "            def _get_all_comments(next_url = None):\n",
    "                comments_url = self.graph_url + post[\"id\"] + \"/comments?\" + self.access_token if next_url is None else next_url    #遞迴用url\n",
    "                comments_json = json.loads(requests.get(comments_url).text)\n",
    "                all_comments_list.extend(comments_json[\"data\"])\n",
    "                try:\n",
    "                    get_all_comments(comments_json[\"paging\"][\"next\"])\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "            _get_all_comments()\n",
    "            post[\"comments\"] = all_comments_list\n",
    "            \n",
    "        print(\"get comments done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read done\n",
      "count: 137\n",
      "2018-06-14T10:37:51+0000\n",
      "2018-06-13T10:05:09+0000\n",
      "2018-06-12T05:47:21+0000\n",
      "2018-06-11T10:26:47+0000\n",
      "2018-06-08T10:27:33+0000\n",
      "2018-06-08T04:00:44+0000\n",
      "2018-06-07T13:07:22+0000\n",
      "2018-06-06T07:06:58+0000\n",
      "2018-06-03T09:01:08+0000\n",
      "2018-06-02T13:53:26+0000\n",
      "2018-06-01T02:44:53+0000\n",
      "2018-05-29T11:10:18+0000\n",
      "2018-05-28T06:35:02+0000\n",
      "2018-05-27T05:54:40+0000\n",
      "2018-05-26T10:44:30+0000\n",
      "2018-05-25T05:54:30+0000\n",
      "2018-05-25T02:14:12+0000\n",
      "2018-05-23T04:30:16+0000\n",
      "2018-05-21T08:18:34+0000\n",
      "2018-05-19T12:45:36+0000\n",
      "2018-05-18T11:08:58+0000\n",
      "2018-05-15T12:21:57+0000\n",
      "2018-05-13T05:51:24+0000\n",
      "2018-05-12T04:01:16+0000\n",
      "2018-05-11T09:53:07+0000\n",
      "2018-05-09T12:03:53+0000\n",
      "2018-05-08T04:00:06+0000\n",
      "2018-05-06T04:47:37+0000\n",
      "2018-05-04T08:16:40+0000\n",
      "2018-05-03T10:02:33+0000\n",
      "2018-05-01T10:50:19+0000\n",
      "2018-05-01T02:08:35+0000\n",
      "2018-04-30T11:16:23+0000\n",
      "2018-04-29T09:21:24+0000\n",
      "2018-04-27T11:34:25+0000\n",
      "2018-04-25T13:30:28+0000\n",
      "2018-04-25T09:51:13+0000\n",
      "2018-04-25T04:30:38+0000\n",
      "2018-04-24T09:31:06+0000\n",
      "2018-04-23T11:24:52+0000\n",
      "2018-04-22T11:44:37+0000\n",
      "2018-04-22T03:05:52+0000\n",
      "2018-04-21T12:06:44+0000\n",
      "2018-04-19T22:53:07+0000\n",
      "2018-04-19T11:28:49+0000\n",
      "2018-04-18T11:22:25+0000\n",
      "2018-04-17T13:01:49+0000\n",
      "2018-04-16T13:47:55+0000\n",
      "2018-04-15T07:38:12+0000\n",
      "2018-04-13T12:39:00+0000\n",
      "2018-04-12T10:46:46+0000\n",
      "2018-04-11T04:26:42+0000\n",
      "2018-04-10T05:09:12+0000\n",
      "2018-04-09T11:26:35+0000\n",
      "2018-04-09T02:10:41+0000\n",
      "2018-04-08T12:09:48+0000\n",
      "2018-04-07T10:09:36+0000\n",
      "2018-04-06T08:38:03+0000\n",
      "2018-04-05T08:29:03+0000\n",
      "2018-04-04T03:20:03+0000\n",
      "2018-04-03T13:08:26+0000\n",
      "2018-04-02T11:18:57+0000\n",
      "2018-04-01T02:08:50+0000\n",
      "2018-03-31T10:30:00+0000\n",
      "2018-03-31T05:10:14+0000\n",
      "2018-03-30T05:30:49+0000\n",
      "2018-03-29T11:34:41+0000\n",
      "2018-03-28T04:40:34+0000\n",
      "2018-03-27T09:20:22+0000\n",
      "2018-03-27T02:00:06+0000\n",
      "2018-03-25T10:54:53+0000\n",
      "2018-03-25T01:58:10+0000\n",
      "2018-03-24T12:27:25+0000\n",
      "2018-03-24T05:18:46+0000\n",
      "2018-03-22T09:00:00+0000\n",
      "2018-03-21T11:25:13+0000\n",
      "2018-03-20T10:17:10+0000\n",
      "2018-03-18T11:07:51+0000\n",
      "2018-03-17T04:36:12+0000\n",
      "2018-03-14T11:13:22+0000\n",
      "2018-03-13T09:46:04+0000\n",
      "2018-03-13T04:33:00+0000\n",
      "2018-03-11T13:42:55+0000\n",
      "2018-03-09T02:19:54+0000\n",
      "2018-03-08T09:00:00+0000\n",
      "2018-03-06T11:55:36+0000\n",
      "2018-03-05T11:00:00+0000\n",
      "2018-03-04T11:58:40+0000\n",
      "2018-03-04T04:32:20+0000\n",
      "2018-03-03T11:30:08+0000\n",
      "2018-03-02T10:15:36+0000\n",
      "2018-03-01T09:43:11+0000\n",
      "2018-02-28T07:45:11+0000\n",
      "2018-02-27T04:33:29+0000\n",
      "2018-02-25T12:12:05+0000\n",
      "2018-02-25T04:09:20+0000\n",
      "2018-02-24T12:38:03+0000\n",
      "2018-02-23T04:04:02+0000\n",
      "2018-02-22T13:02:01+0000\n",
      "2018-02-21T11:30:19+0000\n",
      "2018-02-20T12:08:22+0000\n",
      "2018-02-19T12:05:03+0000\n",
      "2018-02-18T09:12:57+0000\n",
      "2018-02-17T09:00:05+0000\n",
      "2018-02-16T04:00:00+0000\n",
      "2018-02-15T10:31:03+0000\n",
      "2018-02-14T13:11:54+0000\n",
      "2018-02-14T05:00:05+0000\n",
      "2018-02-13T12:39:11+0000\n",
      "2018-02-12T12:58:39+0000\n",
      "2018-02-12T03:35:16+0000\n",
      "2018-02-08T03:51:12+0000\n",
      "2018-02-06T18:47:02+0000\n",
      "2018-02-06T10:28:53+0000\n",
      "2018-02-06T01:00:00+0000\n",
      "2018-02-05T04:12:55+0000\n",
      "2018-02-02T14:40:09+0000\n",
      "2018-02-01T12:13:15+0000\n",
      "2018-02-01T02:30:00+0000\n",
      "2018-01-29T10:07:27+0000\n",
      "2018-01-29T09:40:44+0000\n",
      "2018-01-28T03:30:00+0000\n",
      "2018-01-27T13:15:49+0000\n",
      "2018-01-26T09:29:05+0000\n",
      "2018-01-24T03:57:21+0000\n",
      "2018-01-23T02:22:50+0000\n",
      "2018-01-22T10:50:14+0000\n",
      "2018-01-21T10:57:39+0000\n",
      "2018-01-20T10:10:44+0000\n",
      "2018-01-17T11:23:55+0000\n",
      "2018-01-15T09:23:39+0000\n",
      "2018-01-14T12:51:53+0000\n",
      "2018-01-11T12:33:41+0000\n",
      "2018-01-10T12:29:45+0000\n",
      "2018-01-09T09:30:43+0000\n",
      "2018-01-05T10:44:54+0000\n",
      "2018-01-04T04:33:10+0000\n"
     ]
    }
   ],
   "source": [
    "page_name = \"DoctorKoWJ\"\n",
    "posts_since = \"2018-1-1\"\n",
    "posts_until = \"2018-6-15\"\n",
    "\n",
    "crawler = FbPostCrawler(page_name, posts_since, posts_until)\n",
    "# crawler.getPosts()\n",
    "# crawler.getComments()\n",
    "# crawler.printPosts()\n",
    "# crawler.writePosts()\n",
    "crawler.readPosts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FbLikeCrawler:\n",
    "    def __init__(self, page_name, since=None, until=None):\n",
    "        self.access_token = \"&access_token=\" + \"EAACEdEose0cBAH2GtEDG1E2ZCpziWPRfZCNNEq0sEJFRlCA9mYGaJVgOZBqKRxrESCJvBbkQl9rZAVkomB87Gse2zrMMaXUtdq1FFcZBJqtsrzqNJjhrRFXfAT9ZCQbiAzbtb8uJ0tWZAmHBFJhyt9FfCZCH4SlwNWFZBCxvjLWJXUcoic7JwXHkZAEt4shu6OPh5KSVOndN1lkgZDZD\"\n",
    "        self.graph_url = \"https://graph.facebook.com/v3.0/\"\n",
    "        self.page_url = self.graph_url + \"%s/posts?\" %page_name\n",
    "        self.since, self.until = since, until\n",
    "        try:\n",
    "            self.file_name = \"like_\"+self.since+\"_\"+self.until+\".txt\"    #讀寫檔用\n",
    "        except:\n",
    "            self.file_name = \"like_all.txt\"\n",
    "        self.all_likes = list()    #紀錄所有貼文\n",
    "        \n",
    "    def getLikes(self, next_posts=None):\n",
    "        time_range = \"%s%s\" %(\"\"if self.since is None else \"&since=\"+self.since, \"\"if self.until is None else \"&until=\"+self.until)    #設定起訖時間\n",
    "        post_url = (self.page_url+ self.access_token + time_range) if next_posts is None else next_posts    #遞迴用url\n",
    "#         print(final_url)\n",
    "        \n",
    "        posts_json = json.loads(requests.get(post_url).text)\n",
    "        for data in posts_json[\"data\"]:\n",
    "            tmp_data = {\"created_time\":data[\"created_time\"], \"id\":data[\"id\"]}\n",
    "            like_url = self.graph_url + data[\"id\"] + \"/?fields=likes.summary(true)\" + self.access_token\n",
    "            likes_json = json.loads(requests.get(like_url).text)\n",
    "            tmp_data[\"likes_total_count\"] = likes_json[\"likes\"][\"summary\"][\"total_count\"]\n",
    "            \n",
    "            self.all_likes.append(tmp_data)\n",
    "        if ((next_posts is not None) or ((self.since is not None) and (self.until is not None))):\n",
    "            try:\n",
    "                self.getLikes(posts_json[\"paging\"][\"next\"])    #遞迴直到沒有next\n",
    "            except:\n",
    "                print(\"get likes done\")\n",
    "    \n",
    "    def writeLikes(self):\n",
    "            with open(self.file_name, \"w\") as writer:\n",
    "                writer.write(json.dumps(self.all_likes))\n",
    "            print(\"write done\")\n",
    "    \n",
    "    def readLikes(self, other_file=None):\n",
    "        with open(self.file_name if other_file is None else other_file, \"r\") as reader:\n",
    "            self.all_likes = json.load(reader)\n",
    "        print(\"read done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read done\n",
      "0528,10696\n",
      "https://www.facebook.com/360151611020961_609156979453755\n",
      "0326,9519\n",
      "https://www.facebook.com/360151611020961_578846982484755\n",
      "0325,8135\n",
      "https://www.facebook.com/360151611020961_578314975871289\n",
      "0318,7828\n",
      "https://www.facebook.com/360151611020961_574309309605189\n",
      "0330,7609\n",
      "https://www.facebook.com/360151611020961_580319172337536\n"
     ]
    }
   ],
   "source": [
    "page_name = \"professorofpower\"\n",
    "posts_since = \"2018-1-1\"\n",
    "posts_until = \"2018-6-15\"\n",
    "like_crawler = FbLikeCrawler(page_name, posts_since, posts_until)\n",
    "# like_crawler.getLikes()\n",
    "# like_crawler.writeLikes()\n",
    "like_crawler.readLikes(\"kun_like_2018-1-1_2018-6-15.txt\")\n",
    "like_rank = sorted(like_crawler.all_likes, key=lambda k: k['likes_total_count'], reverse=True)\n",
    "for post in like_rank[:5]:\n",
    "    print(post[\"created_time\"][5:7]+post[\"created_time\"][8:10] +\",\"+ str(post[\"likes_total_count\"]))\n",
    "    print(\"https://www.facebook.com/\"+post[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
