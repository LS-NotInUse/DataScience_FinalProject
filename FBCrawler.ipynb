{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_page_name = [\"DoctorKoWJ\", \"YaoTurningTaipei\", \"tingshouchung\", \"professorofpower\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "\n",
    "class FbPostCrawler:\n",
    "    def __init__(self, page_name, since=None, until=None):\n",
    "        self.access_token = \"&access_token=\" + \"EAACEdEose0cBABwwmb8zl7alxwGVyzqYEl6loWPocUsHwZBHViGZB1BYZB4p0ZBF9KF9dnZA8ZBj1zxQHAZBXo6yuxpJ3BnViMRnTMKSZCVddkYpfkwQsQibVCZAtTiZAABbWRSW2esC2cKoyURID1lXZAu8IsU4JtvP3NLozrq94DqdsfVPvm64rE8EplTEes09AnotUakunvmJgZDZD\"\n",
    "        self.graph_url = \"https://graph.facebook.com/v3.0/\"\n",
    "        self.page_url = self.graph_url + \"%s/posts?\" %page_name\n",
    "        self.since, self.until = since, until\n",
    "        try:\n",
    "            self.file_name = page_name+\"_post_\"+self.since+\"_\"+self.until+\".txt\"    #讀寫檔用\n",
    "        except:\n",
    "            self.file_name = \"post_all.txt\"\n",
    "        self.all_posts = list()    #紀錄所有貼文\n",
    "        \n",
    "    def getPosts(self, next_posts=None):\n",
    "#         print(\"alive\")\n",
    "        time_range = \"%s%s\" %(\"\"if self.since is None else \"&since=\"+self.since, \"\"if self.until is None else \"&until=\"+self.until)    #設定起訖時間\n",
    "        post_url = (self.page_url+ self.access_token + time_range) if next_posts is None else next_posts    #遞迴用url\n",
    "        \n",
    "        posts_json = json.loads(requests.get(post_url).text)\n",
    "        for data in posts_json[\"data\"]:\n",
    "            self.all_posts.append(data)\n",
    "            \n",
    "        if ((next_posts is not None) or ((self.since is not None) and (self.until is not None))):\n",
    "            try:\n",
    "                self.getPosts(posts_json[\"paging\"][\"next\"])    #遞迴直到沒有next\n",
    "            except:\n",
    "                print(\"get post done\")\n",
    "        \n",
    "    def printPosts(self):\n",
    "        if(len(self.all_posts) == 0):\n",
    "            print(\"empty\")\n",
    "            return\n",
    "        print(\"count: \" + str(len(self.all_posts)))\n",
    "        for element in self.all_posts:\n",
    "            print(element[\"created_time\"])\n",
    "    \n",
    "    def writePosts(self):\n",
    "            with open(self.file_name, \"w\") as writer:\n",
    "                writer.write(json.dumps(self.all_posts))\n",
    "            print(\"write post done\")\n",
    "    \n",
    "    def readPosts(self, other_file=None):\n",
    "        with open(self.file_name if other_file is None else other_file, \"r\") as reader:\n",
    "            self.all_posts = json.load(reader)\n",
    "        print(\"read post done\")\n",
    "        \n",
    "#######################################################\n",
    "    def getComments(self, next_comments= None):\n",
    "        for post in self.all_posts:\n",
    "            all_comments_list = list()\n",
    "            \n",
    "            def _get_all_comments(next_url = None):\n",
    "                comments_url = self.graph_url + post[\"id\"] + \"/comments?\" + self.access_token if next_url is None else next_url    #遞迴用url\n",
    "                comments_json = json.loads(requests.get(comments_url).text)\n",
    "                all_comments_list.extend(comments_json[\"data\"])\n",
    "                try:\n",
    "                    get_all_comments(comments_json[\"paging\"][\"next\"])\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "            _get_all_comments()\n",
    "            post[\"comments\"] = all_comments_list\n",
    "            \n",
    "        print(\"get comments done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_name = candidate_page_name[0]\n",
    "posts_since = \"2018-1-1\"\n",
    "posts_until = \"2018-6-15\"\n",
    "\n",
    "post_crawler = FbPostCrawler(page_name, posts_since, posts_until)\n",
    "# post_crawler.getPosts()\n",
    "# post_crawler.getComments()\n",
    "# crawler.printPosts()\n",
    "# crawler.writePosts()\n",
    "# crawler.readPosts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FbReactionCrawler:\n",
    "    def __init__(self, page_name, since=None, until=None):\n",
    "        self.access_token = \"&access_token=\" + \"EAACEdEose0cBABwwmb8zl7alxwGVyzqYEl6loWPocUsHwZBHViGZB1BYZB4p0ZBF9KF9dnZA8ZBj1zxQHAZBXo6yuxpJ3BnViMRnTMKSZCVddkYpfkwQsQibVCZAtTiZAABbWRSW2esC2cKoyURID1lXZAu8IsU4JtvP3NLozrq94DqdsfVPvm64rE8EplTEes09AnotUakunvmJgZDZD\"\n",
    "        self.graph_url = \"https://graph.facebook.com/v3.0/\"\n",
    "        self.page_url = self.graph_url + \"%s/posts?\" %page_name\n",
    "        self.since, self.until = since, until\n",
    "        try:\n",
    "            self.file_name = page_name+\"_reaction_\"+self.since+\"_\"+self.until+\".txt\"    #讀寫檔用\n",
    "        except:\n",
    "            self.file_name = page_name+\"_reaction_all.txt\"\n",
    "        self.all_reactions = list()    #紀錄所有貼文\n",
    "        \n",
    "    def getReactions(self, next_posts=None):\n",
    "        print(\"alive\")\n",
    "        time_range = \"%s%s\" %(\"\"if self.since is None else \"&since=\"+self.since, \"\"if self.until is None else \"&until=\"+self.until)    #設定起訖時間\n",
    "        post_url = (self.page_url+ self.access_token + time_range) if next_posts is None else next_posts    #遞迴用url\n",
    "        posts_json = json.loads(requests.get(post_url).text)\n",
    "        \n",
    "        for data in posts_json[\"data\"]:\n",
    "            reaction_url = self.graph_url + data[\"id\"] + \"/?fields=reactions.type(NONE).limit(0).summary(1).as(ALL),reactions.type(LIKE).limit(0).summary(1).as(LIKE),reactions.type(LOVE).limit(0).summary(1).as(LOVE),reactions.type(HAHA).limit(0).summary(1).as(HAHA),reactions.type(WOW).limit(0).summary(1).as(WOW),reactions.type(SAD).limit(0).summary(1).as(SAD),reactions.type(ANGRY).limit(0).summary(1).as(ANGRY)\" +self.access_token\n",
    "            reaction_json = json.loads(requests.get(reaction_url).text)\n",
    "            self.all_reactions.append({\"created_time\":data[\"created_time\"], \"id\":data[\"id\"],\n",
    "                                        \"reactions\":[reaction_json[\"ALL\"][\"summary\"][\"total_count\"],\n",
    "                                                   reaction_json[\"LIKE\"][\"summary\"][\"total_count\"],\n",
    "                                                   reaction_json[\"LOVE\"][\"summary\"][\"total_count\"],\n",
    "                                                   reaction_json[\"HAHA\"][\"summary\"][\"total_count\"],\n",
    "                                                   reaction_json[\"WOW\"][\"summary\"][\"total_count\"],\n",
    "                                                   reaction_json[\"SAD\"][\"summary\"][\"total_count\"],\n",
    "                                                   reaction_json[\"ANGRY\"][\"summary\"][\"total_count\"]]})\n",
    "        if ((next_posts is not None) or ((self.since is not None) and (self.until is not None))):\n",
    "            try:\n",
    "                self.getReactions(posts_json[\"paging\"][\"next\"])    #遞迴直到沒有next\n",
    "            except:\n",
    "                print(\"get reacition done\")\n",
    "    \n",
    "    def writeReactions(self):\n",
    "            with open(self.file_name, \"w\") as writer:\n",
    "                writer.write(json.dumps(self.all_reactions))\n",
    "            print(\"write reaction done\")\n",
    "    \n",
    "    def readReactions(self, other_file=None):\n",
    "        with open(self.file_name if other_file is None else other_file, \"r\") as reader:\n",
    "            self.all_reactions = json.load(reader)\n",
    "        print(\"read reaction done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "page_name = candidate_page_name[0]\n",
    "posts_since = \"2018-1-1\"\n",
    "posts_until = \"2018-6-15\"\n",
    "\n",
    "reaction_crawler = FbReactionCrawler(page_name, posts_since, posts_until)\n",
    "# reaction_crawler.getReactions()\n",
    "# reaction_crawler.writeReactions()\n",
    "# reaction_crawler.readReactions()\n",
    "\n",
    "reaction_rank = sorted(reaction_crawler.all_reactions, key=lambda k: k[\"reactions\"][0], reverse=True)\n",
    "for post in reaction_rank[:5]:\n",
    "    print(post[\"created_time\"][5:7]+post[\"created_time\"][8:10] +\",\"+ str(post[\"reactions\"]))\n",
    "    print(\"https://www.facebook.com/\"+post[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FbCountCrawler:\n",
    "    def __init__(self, page_name, since=None, until=None):\n",
    "        self.access_token = \"&access_token=\" + \"EAACEdEose0cBABwwmb8zl7alxwGVyzqYEl6loWPocUsHwZBHViGZB1BYZB4p0ZBF9KF9dnZA8ZBj1zxQHAZBXo6yuxpJ3BnViMRnTMKSZCVddkYpfkwQsQibVCZAtTiZAABbWRSW2esC2cKoyURID1lXZAu8IsU4JtvP3NLozrq94DqdsfVPvm64rE8EplTEes09AnotUakunvmJgZDZD\"\n",
    "        self.graph_url = \"https://graph.facebook.com/v3.0/\"\n",
    "        self.page_url = self.graph_url + \"%s/posts?\" %page_name\n",
    "        self.since, self.until = since, until\n",
    "        try:\n",
    "            self.file_name = page_name+\"_count_\"+self.since+\"_\"+self.until+\".txt\"    #讀寫檔用\n",
    "        except:\n",
    "            self.file_name = page_name+\"_count_all.txt\"\n",
    "        self.all_counts = list()    #紀錄所有貼文\n",
    "        \n",
    "    def getCounts(self, next_posts=None):\n",
    "        print(\"alive\")\n",
    "        time_range = \"%s%s\" %(\"\"if self.since is None else \"&since=\"+self.since, \"\"if self.until is None else \"&until=\"+self.until)    #設定起訖時間\n",
    "        post_url = (self.page_url+ self.access_token + time_range) if next_posts is None else next_posts    #遞迴用url \n",
    "        posts_json = json.loads(requests.get(post_url).text)\n",
    "        \n",
    "        for data in posts_json[\"data\"]:\n",
    "            count_url = self.graph_url + data[\"id\"] + \"/?fields=shares,comments.limit(0).summary(1)\" + self.access_token\n",
    "            count_json = json.loads(requests.get(count_url).text)\n",
    "            self.all_counts.append({\"created_time\":data[\"created_time\"], \"id\":data[\"id\"], \"comments_count\":count_json[\"comments\"][\"summary\"][\"total_count\"], \"shares_count\":0 if \"shares\" not in count_json else count_json[\"shares\"][\"count\"]})\n",
    "            \n",
    "        if ((next_posts is not None) or ((self.since is not None) and (self.until is not None))):\n",
    "            try:\n",
    "                self.getCounts(posts_json[\"paging\"][\"next\"])    #遞迴直到沒有next\n",
    "            except:\n",
    "                print(\"get count done\")\n",
    "    \n",
    "    def writeCounts(self):\n",
    "            with open(self.file_name, \"w\") as writer:\n",
    "                writer.write(json.dumps(self.all_counts))\n",
    "            print(\"write count done\")\n",
    "    \n",
    "    def readCounts(self, other_file=None):\n",
    "        with open(self.file_name if other_file is None else other_file, \"r\") as reader:\n",
    "            self.all_counts = json.load(reader)\n",
    "        print(\"read count done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_name = candidate_page_name[3]\n",
    "posts_since = \"2018-1-1\"\n",
    "posts_until = \"2018-6-15\"\n",
    "\n",
    "count_crawler = FbCountCrawler(page_name, posts_since, posts_until)\n",
    "count_crawler.getCounts()\n",
    "count_crawler.writeCounts()\n",
    "count_crawler.readCounts()\n",
    "\n",
    "comment_rank = sorted(count_crawler.all_counts, key=lambda k: k[\"comments_count\"], reverse=True)\n",
    "share_rank = sorted(count_crawler.all_counts, key=lambda k: k[\"shares_count\"], reverse=True)\n",
    "print(\"comment:\")\n",
    "for post in comment_rank[:5]:\n",
    "    print(post[\"created_time\"][5:7]+post[\"created_time\"][8:10] +\",\"+ str(post[\"comments_count\"]))\n",
    "    print(\"https://www.facebook.com/\"+post[\"id\"])\n",
    "print(\"\\nshare:\")\n",
    "for post in share_rank[:5]:\n",
    "    print(post[\"created_time\"][5:7]+post[\"created_time\"][8:10] +\",\"+ str(post[\"shares_count\"]))\n",
    "    print(\"https://www.facebook.com/\"+post[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
